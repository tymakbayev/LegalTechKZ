"""
–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å adilet.zan.kz
"""

from typing import Dict, Any
import logging
import requests
from bs4 import BeautifulSoup

from legaltechkz.agents.tools.base_tool import BaseTool
from legaltechkz.expertise.document_parser import DocumentParser

logger = logging.getLogger("legaltechkz.agents.tools.document_fetch")


class DocumentFetchTool(BaseTool):
    """
    –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–≥–¥–∞ –∞–≥–µ–Ω—Ç –Ω–∞—à–µ–ª –¥–æ–∫—É–º–µ–Ω—Ç –∏ —Ö–æ—á–µ—Ç:
    - –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
    - –ò–∑–≤–ª–µ—á—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Å—Ç–∞—Ç—å—é
    - –ü–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
    """

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞."""
        self.parser = DocumentParser()
        self.session = requests.Session()
        super().__init__()

    def get_name(self) -> str:
        return "fetch_document"

    def get_description(self) -> str:
        return """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å adilet.zan.kz

–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ:
- –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
- –ò–∑–≤–ª–µ—á—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Å—Ç–∞—Ç—å—é –∏–∑ –¥—Ä—É–≥–æ–≥–æ –∑–∞–∫–æ–Ω–∞
- –ü–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–≥–ª–∞–≤—ã, —Å—Ç–∞—Ç—å–∏, –ø—É–Ω–∫—Ç—ã)

–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
- url (str): URL –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ adilet.zan.kz
- article_number (int, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ): –ù–æ–º–µ—Ä —Å—Ç–∞—Ç—å–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (–µ—Å–ª–∏ –Ω—É–∂–Ω–∞ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ —Å—Ç–∞—Ç—å—è)

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏"""

    def run(self, url: str, article_number: int = None) -> Dict[str, Any]:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç.

        Args:
            url: URL –¥–æ–∫—É–º–µ–Ω—Ç–∞
            article_number: –ù–æ–º–µ—Ä —Å—Ç–∞—Ç—å–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

        Returns:
            –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        """
        logger.info(f"üìÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {url}")

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            response = self.session.get(url, verify=False, timeout=15)
            response.raise_for_status()

            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(response.content, 'html.parser')

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
            title_elem = soup.find('h1') or soup.find('title')
            title = title_elem.get_text(strip=True) if title_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
            content_div = soup.find('div', {'class': 'document'}) or soup.find('div', {'id': 'content'})
            if not content_div:
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ª—é–±–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –±–æ–ª—å—à–∏–º —Ç–µ–∫—Å—Ç–æ–º
                content_div = soup.find('body')

            if not content_div:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞")

            full_text = content_div.get_text(separator='\n', strip=True)

            # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            parsed = self.parser.parse_document(full_text, title)

            # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Å—Ç–∞—Ç—å—è
            if article_number is not None:
                article = self._find_article(parsed['fragments'], article_number)
                if article:
                    logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∞ —Å—Ç–∞—Ç—å—è {article_number}")
                    return {
                        "success": True,
                        "url": url,
                        "title": title,
                        "article_number": article_number,
                        "article_text": article['text'],
                        "article_path": article['full_path'],
                        "message": f"–ò–∑–≤–ª–µ—á–µ–Ω–∞ {article['full_path']}"
                    }
                else:
                    return {
                        "success": False,
                        "url": url,
                        "message": f"–°—Ç–∞—Ç—å—è {article_number} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ"
                    }

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {parsed['articles_count']} —Å—Ç–∞—Ç–µ–π")

            return {
                "success": True,
                "url": url,
                "title": title,
                "articles_count": parsed['articles_count'],
                "fragments_count": parsed['fragments_count'],
                "fragments": parsed['fragments'][:10],  # –ü–µ—Ä–≤—ã–µ 10 —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
                "table_of_contents": parsed['table_of_contents'][:1000],  # –ü–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è
                "message": f"–ó–∞–≥—Ä—É–∂–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç '{title}': {parsed['articles_count']} —Å—Ç–∞—Ç–µ–π"
            }

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return {
                "success": False,
                "url": url,
                "error": str(e),
                "message": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}"
            }

    def _find_article(self, fragments, article_number):
        """–ù–∞–π—Ç–∏ —Å—Ç–∞—Ç—å—é –ø–æ –Ω–æ–º–µ—Ä—É."""
        for fragment in fragments:
            if fragment.type == 'article' and fragment.number == article_number:
                return {
                    'text': fragment.text,
                    'full_path': fragment.full_path,
                    'number': fragment.number
                }
        return None
