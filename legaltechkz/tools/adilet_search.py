"""
–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø–æ–∏—Å–∫–∞ –ø–æ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –ù–ü–ê –†–ö –Ω–∞ adilet.zan.kz

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Google Search —Å –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º site:adilet.zan.kz –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
"""

import logging
import os
import requests
from typing import Dict, Any, Union, List, Optional
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, quote
import urllib3

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –Ω–µ–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö HTTPS –∑–∞–ø—Ä–æ—Å–∞—Ö –¥–ª—è adilet.zan.kz
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

from legaltechkz.tools.base.tool import BaseTool
from legaltechkz.tools.base.tool_result import ToolResult

logger = logging.getLogger(__name__)


class AdiletSearchTool(BaseTool):
    """
    –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –ù–ü–ê –Ω–∞ adilet.zan.kz

    –ü–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–∫–∞—Ç—å:
    - –ó–∞–∫–æ–Ω—ã –†–ö
    - –ö–æ–¥–µ–∫—Å—ã
    - –£–∫–∞–∑—ã –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞
    - –ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞
    - –ü—Ä–∏–∫–∞–∑—ã –º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤
    - –î—Ä—É–≥–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ-–ø—Ä–∞–≤–æ–≤—ã–µ –∞–∫—Ç—ã
    """

    name = "adilet_search"
    description = "–ü–æ–∏—Å–∫ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ-–ø—Ä–∞–≤–æ–≤—ã—Ö –∞–∫—Ç–æ–≤ –†–ö –Ω–∞ adilet.zan.kz"
    parameters = {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–Ω–∞–∑–≤–∞–Ω–∏–µ –∑–∞–∫–æ–Ω–∞, –Ω–æ–º–µ—Ä, –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)"
            },
            "doc_type": {
                "type": "string",
                "description": "–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: law (–∑–∞–∫–æ–Ω), code (–∫–æ–¥–µ–∫—Å), decree (—É–∫–∞–∑), resolution (–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ), order (–ø—Ä–∏–∫–∞–∑)",
                "enum": ["law", "code", "decree", "resolution", "order", "all"]
            },
            "year": {
                "type": "string",
                "description": "–ì–æ–¥ –ø—Ä–∏–Ω—è—Ç–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)"
            },
            "status": {
                "type": "string",
                "description": "–°—Ç–∞—Ç—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞: active (–¥–µ–π—Å—Ç–≤—É—é—â–∏–π), invalid (—É—Ç—Ä–∞—Ç–∏–≤—à–∏–π —Å–∏–ª—É)",
                "enum": ["active", "invalid", "all"]
            }
        },
        "required": ["query"]
    }

    # –ë–∞–∑–æ–≤—ã–π URL adilet.zan.kz
    BASE_URL = "https://adilet.zan.kz"
    SEARCH_URL = f"{BASE_URL}/rus/search/docs"

    # –¢–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ adilet.zan.kz
    DOC_TYPES = {
        "law": "–ó–∞–∫–æ–Ω",
        "code": "–ö–æ–¥–µ–∫—Å",
        "decree": "–£–∫–∞–∑",
        "resolution": "–ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ",
        "order": "–ü—Ä–∏–∫–∞–∑",
        "all": ""
    }

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –ø–æ–∏—Å–∫–∞ Adilet"""
        super().__init__()
        self.session = requests.Session()
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã –æ—Ç –±–æ—Ç–æ–≤
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Å—Å–∏—é, –ø–æ–ª—É—á–∏–≤ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        self._init_session()

    def _init_session(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–µ—Å—Å–∏—é, –ø–æ—Å–µ—Ç–∏–≤ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É adilet.zan.kz"""
        try:
            logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏ —Å adilet.zan.kz")
            # verify=False –¥–ª—è –æ–±—Ö–æ–¥–∞ –ø—Ä–æ–±–ª–µ–º —Å SSL —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–º adilet.zan.kz
            response = self.session.get(f"{self.BASE_URL}/rus", timeout=10, verify=False)
            if response.status_code == 200:
                logger.info("–°–µ—Å—Å–∏—è —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º cookies –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                return True
            else:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–µ—Å—Å–∏—é: {response.status_code}")
                return False
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–µ—Å—Å–∏–∏: {e}")
            return False

    def execute(
        self,
        query: str,
        doc_type: str = "all",
        year: Optional[str] = None,
        status: str = "active",
        **kwargs
    ) -> Union[Dict[str, Any], ToolResult]:
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫ –Ω–∞ adilet.zan.kz

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            doc_type: –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
            year: –ì–æ–¥ –ø—Ä–∏–Ω—è—Ç–∏—è
            status: –°—Ç–∞—Ç—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–¥–µ–π—Å—Ç–≤—É—é—â–∏–π/—É—Ç—Ä–∞—Ç–∏–≤—à–∏–π —Å–∏–ª—É)
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ù–ü–ê
        """
        try:
            logger.info(f"–ü–æ–∏—Å–∫ –ù–ü–ê –Ω–∞ adilet.zan.kz: '{query}'")

            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
            search_params = self._build_search_params(query, doc_type, year, status)

            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            results = self._perform_search(search_params)

            if not results:
                logger.warning(f"–î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω –ª–∏ Google Custom Search API
                google_api_key = os.environ.get("GOOGLE_CUSTOM_SEARCH_API_KEY")
                google_cx = os.environ.get("GOOGLE_CUSTOM_SEARCH_CX")

                if not google_api_key or not google_cx:
                    message = (
                        "‚ö†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –í–æ–∑–º–æ–∂–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞: —Å–∞–π—Ç adilet.zan.kz –±–ª–æ–∫–∏—Ä—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã.\n\n"
                        "üí° –†–ï–®–ï–ù–ò–ï: –ù–∞—Å—Ç—Ä–æ–π—Ç–µ Google Custom Search API –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞:\n"
                        "1. –°–æ–∑–¥–∞–π—Ç–µ Custom Search Engine: https://programmablesearchengine.google.com/\n"
                        "2. –ü–æ–ª—É—á–∏—Ç–µ API –∫–ª—é—á: https://console.cloud.google.com/apis/credentials\n"
                        "3. –î–æ–±–∞–≤—å—Ç–µ –≤ .env —Ñ–∞–π–ª:\n"
                        "   GOOGLE_CUSTOM_SEARCH_API_KEY=–≤–∞—à_–∫–ª—é—á\n"
                        "   GOOGLE_CUSTOM_SEARCH_CX=–≤–∞—à_search_engine_id\n\n"
                        "üìñ –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: docs/GOOGLE_CUSTOM_SEARCH_SETUP.md\n"
                        "üéÅ –ë–µ—Å–ø–ª–∞—Ç–Ω–æ: 100 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –¥–µ–Ω—å"
                    )
                else:
                    message = "–î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞."

                return {
                    "status": "success",
                    "query": query,
                    "results": [],
                    "result_count": 0,
                    "message": message
                }

            logger.info(f"–ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(results)}")

            return {
                "status": "success",
                "query": query,
                "doc_type": doc_type,
                "year": year,
                "status_filter": status,
                "results": results,
                "result_count": len(results),
                "source": "adilet.zan.kz"
            }

        except requests.RequestException as e:
            error_msg = f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ adilet.zan.kz: {str(e)}"
            logger.error(error_msg)
            return {
                "status": "error",
                "error": error_msg,
                "suggestion": "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å adilet.zan.kz"
            }
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {str(e)}"
            logger.error(error_msg)
            return {
                "status": "error",
                "error": error_msg
            }

    def _build_search_params(
        self,
        query: str,
        doc_type: str,
        year: Optional[str],
        status: str
    ) -> Dict[str, Any]:
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è adilet.zan.kz

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            doc_type: –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
            year: –ì–æ–¥ –ø—Ä–∏–Ω—è—Ç–∏—è
            status: –°—Ç–∞—Ç—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ–∏—Å–∫–∞
        """
        params = {
            "q": query,
            "lang": "ru"
        }

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
        if doc_type != "all" and doc_type in self.DOC_TYPES:
            params["type"] = self.DOC_TYPES[doc_type]

        # –î–æ–±–∞–≤–ª—è–µ–º –≥–æ–¥
        if year:
            params["year"] = year

        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        if status == "active":
            params["valid"] = "1"
        elif status == "invalid":
            params["valid"] = "0"

        return params

    def _perform_search(self, params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google Custom Search API –∏–ª–∏ –Ω–∞–ø—Ä—è–º—É—é –Ω–∞ adilet.zan.kz

        Args:
            params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞

        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        query = params.get("q", "")
        logger.info(f"–í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫: '{query}'")

        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º Google Custom Search API, –µ—Å–ª–∏ –µ—Å—Ç—å –∫–ª—é—á
        google_api_key = os.environ.get("GOOGLE_CUSTOM_SEARCH_API_KEY")
        google_cx = os.environ.get("GOOGLE_CUSTOM_SEARCH_CX")

        if google_api_key and google_cx:
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º Google Custom Search API")
            results = self._google_custom_search(query, params, google_api_key, google_cx)
            if results:
                return results
            logger.warning("Google Custom Search API –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π –ø–æ–∏—Å–∫")

        # –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –Ω–∞ adilet.zan.kz
        return self._direct_adilet_search(params)

    def _parse_google_results(self, html: str, status_filter: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        –†–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Google Search

        Args:
            html: HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Google
            status_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Å—Ç–∞—Ç—É—Å—É (1 - –¥–µ–π—Å—Ç–≤—É—é—â–∏–π, 0 - —É—Ç—Ä–∞—Ç–∏–≤—à–∏–π —Å–∏–ª—É)

        Returns:
            –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        try:
            soup = BeautifulSoup(html, 'html.parser')
            results = []

            # Google –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–Ω—ã–µ –∫–ª–∞—Å—Å—ã –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –±–ª–æ–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            search_results = soup.find_all('div', class_='g')

            logger.info(f"–ù–∞–π–¥–µ–Ω–æ –±–ª–æ–∫–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Google: {len(search_results)}")

            for result in search_results[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
                try:
                    # –ò—â–µ–º —Å—Å—ã–ª–∫—É
                    link_elem = result.find('a', href=True)
                    if not link_elem:
                        continue

                    url = link_elem.get('href', '')

                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏ –Ω–∞ adilet.zan.kz/rus/docs/
                    if 'adilet.zan.kz/rus/docs/' not in url:
                        continue

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    title_elem = result.find('h3')
                    title = title_elem.get_text(strip=True) if title_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ (snippet)
                    snippet_elem = result.find('div', class_=['VwiC3b', 'IsZvec', 'aCOpRe'])
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –∏ –¥–∞—Ç—É –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏–ª–∏ snippet
                    doc_number = self._extract_doc_number(title + " " + snippet)
                    doc_date = self._extract_date_from_text(title + " " + snippet)

                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
                    full_text = (title + " " + snippet).lower()
                    if "—É—Ç—Ä–∞—Ç–∏–ª" in full_text or "–Ω–µ–¥–µ–π—Å—Ç–≤—É—é—â" in full_text or "–ø—Ä–∏–∑–Ω–∞–Ω —É—Ç—Ä–∞—Ç–∏–≤—à–∏–º" in full_text:
                        status = "–£—Ç—Ä–∞—Ç–∏–ª —Å–∏–ª—É"
                    else:
                        status = "–î–µ–π—Å—Ç–≤—É–µ—Ç"

                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ —Å—Ç–∞—Ç—É—Å—É –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
                    if status_filter:
                        if status_filter == "1" and status != "–î–µ–π—Å—Ç–≤—É–µ—Ç":
                            continue
                        elif status_filter == "0" and status != "–£—Ç—Ä–∞—Ç–∏–ª —Å–∏–ª—É":
                            continue

                    doc_info = {
                        "title": title,
                        "url": url,
                        "number": doc_number or "–ù–µ —É–∫–∞–∑–∞–Ω",
                        "date": doc_date or "–ù–µ —É–∫–∞–∑–∞–Ω–∞",
                        "status": status,
                        "source": "adilet.zan.kz (—á–µ—Ä–µ–∑ Google Search)",
                        "snippet": snippet[:200] if snippet else None
                    }

                    results.append(doc_info)
                    logger.debug(f"–î–æ–±–∞–≤–ª–µ–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {title[:50]}...")

                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ Google: {e}")
                    continue

            return results

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Google: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []

    def _google_custom_search(
        self,
        query: str,
        params: Dict[str, Any],
        api_key: str,
        cx: str
    ) -> List[Dict[str, Any]]:
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google Custom Search JSON API

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            params: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
            api_key: API –∫–ª—é—á Google Custom Search
            cx: Custom Search Engine ID

        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å —Å site: –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º
            search_query = f"{query} site:adilet.zan.kz"

            # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
            doc_type = params.get("type")
            if doc_type:
                search_query += f" {doc_type}"

            year = params.get("year")
            if year:
                search_query += f" {year}"

            # Google Custom Search JSON API endpoint
            api_url = "https://www.googleapis.com/customsearch/v1"
            api_params = {
                "key": api_key,
                "cx": cx,
                "q": search_query,
                "num": 10,  # –ú–∞–∫—Å–∏–º—É–º 10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                "lr": "lang_ru",  # –†—É—Å—Å–∫–∏–π —è–∑—ã–∫
            }

            logger.info(f"Google Custom Search –∑–∞–ø—Ä–æ—Å: '{search_query}'")

            response = self.session.get(api_url, params=api_params, timeout=15)
            response.raise_for_status()

            data = response.json()
            results = []

            if "items" in data:
                for item in data["items"]:
                    url = item.get("link", "")

                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã adilet
                    if "adilet.zan.kz/rus/docs/" not in url:
                        continue

                    title = item.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")
                    snippet = item.get("snippet", "")

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    doc_number = self._extract_doc_number(title + " " + snippet)
                    doc_date = self._extract_date_from_text(title + " " + snippet)

                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
                    full_text = (title + " " + snippet).lower()
                    if "—É—Ç—Ä–∞—Ç–∏–ª" in full_text or "–Ω–µ–¥–µ–π—Å—Ç–≤—É—é—â" in full_text:
                        status = "–£—Ç—Ä–∞—Ç–∏–ª —Å–∏–ª—É"
                    else:
                        status = "–î–µ–π—Å—Ç–≤—É–µ—Ç"

                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ —Å—Ç–∞—Ç—É—Å—É
                    status_filter = params.get("valid")
                    if status_filter:
                        if status_filter == "1" and status != "–î–µ–π—Å—Ç–≤—É–µ—Ç":
                            continue
                        elif status_filter == "0" and status != "–£—Ç—Ä–∞—Ç–∏–ª —Å–∏–ª—É":
                            continue

                    doc_info = {
                        "title": title,
                        "url": url,
                        "number": doc_number or "–ù–µ —É–∫–∞–∑–∞–Ω",
                        "date": doc_date or "–ù–µ —É–∫–∞–∑–∞–Ω–∞",
                        "status": status,
                        "source": "adilet.zan.kz (—á–µ—Ä–µ–∑ Google Custom Search API)",
                        "snippet": snippet[:200] if snippet else None
                    }

                    results.append(doc_info)
                    logger.debug(f"–î–æ–±–∞–≤–ª–µ–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {title[:50]}...")

                logger.info(f"–ù–∞–π–¥–µ–Ω–æ —á–µ—Ä–µ–∑ Google Custom Search: {len(results)}")
                return results

            logger.warning("Google Custom Search –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return []

        except requests.RequestException as e:
            logger.error(f"–û—à–∏–±–∫–∞ Google Custom Search API: {e}")
            return []
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ Google Custom Search: {e}")
            return []

    def _direct_adilet_search(self, params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –Ω–∞ adilet.zan.kz (–∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç)

        Args:
            params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞

        Returns:
            –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –Ω–∞ adilet.zan.kz")

        try:
            import time
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            time.sleep(0.5)

            # –î–æ–±–∞–≤–ª—è–µ–º Referer –¥–ª—è –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            headers = {
                'Referer': f"{self.BASE_URL}/rus"
            }

            response = self.session.get(
                self.SEARCH_URL,
                params=params,
                headers=headers,
                timeout=15,
                allow_redirects=True,
                verify=False  # –û–±—Ö–æ–¥ –ø—Ä–æ–≤–µ—Ä–∫–∏ SSL –¥–ª—è adilet.zan.kz
            )

            logger.info(f"–û—Ç–≤–µ—Ç –æ—Ç adilet.zan.kz: —Å—Ç–∞—Ç—É—Å {response.status_code}, URL: {response.url}")

            if response.status_code == 403:
                logger.error("–ü–æ–ª—É—á–µ–Ω 403 Forbidden - —Å–∞–π—Ç –±–ª–æ–∫–∏—Ä—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã")
                logger.info("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ Google Custom Search API –¥–ª—è –æ–±—Ö–æ–¥–∞ —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã")
                return []

            response.raise_for_status()
            results = self._parse_search_results(response.text)

            if results:
                logger.info(f"–ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –Ω–∞—à–µ–ª {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            else:
                logger.warning("–ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –Ω–µ –Ω–∞—à–µ–ª –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

            return results if results else []

        except requests.HTTPError as e:
            if e.response.status_code == 403:
                logger.error("‚ùå –°–∞–π—Ç adilet.zan.kz –±–ª–æ–∫–∏—Ä—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã (403)")
                logger.info("üí° –†–µ—à–µ–Ω–∏–µ: –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ Google Custom Search API –≤ .env —Ñ–∞–π–ª–µ")
                logger.info("   GOOGLE_CUSTOM_SEARCH_API_KEY –∏ GOOGLE_CUSTOM_SEARCH_CX")
            else:
                logger.error(f"HTTP –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä—è–º–æ–º –ø–æ–∏—Å–∫–µ: {e}")
            return []
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä—è–º–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []

    def _parse_search_results(self, html: str) -> List[Dict[str, Any]]:
        """
        –†–∞—Å–ø–∞—Ä—Å–∏—Ç—å HTML —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞

        Args:
            html: HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏

        Returns:
            –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        try:
            soup = BeautifulSoup(html, 'html.parser')
            results = []

            # –ò—â–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ adilet.zan.kz
            search_items = (
                soup.find_all('div', class_='search-result-item') or
                soup.find_all('div', class_='document-item') or
                soup.find_all('div', class_='doc-item') or
                soup.find_all('tr', class_='document-row') or
                soup.find_all('li', class_='result')
            )

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∫–ª–∞—Å—Å—ã, –∏—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            if not search_items:
                logger.info("–ù–µ –Ω–∞—à–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã, –∏—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã")
                # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –≤–µ–¥—É—Ç –Ω–∞ /rus/docs/
                links = soup.find_all('a', href=re.compile(r'/rus/docs/[A-Z]'))
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã: {len(links)}")

                seen_urls = set()
                for link in links[:20]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ 20 —Å—Å—ã–ª–æ–∫
                    doc_info = self._extract_from_link(link)
                    if doc_info and doc_info['url'] not in seen_urls:
                        results.append(doc_info)
                        seen_urls.add(doc_info['url'])
                        if len(results) >= 10:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
                            break
            else:
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(search_items)}")
                for item in search_items[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
                    doc_info = self._extract_document_info(item)
                    if doc_info:
                        results.append(doc_info)

            logger.info(f"–ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞: {len(results)}")
            return results

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []

    def _extract_document_info(self, item) -> Optional[Dict[str, Any]]:
        """
        –ò–∑–≤–ª–µ—á—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –∏–∑ HTML —ç–ª–µ–º–µ–Ω—Ç–∞

        Args:
            item: HTML —ç–ª–µ–º–µ–Ω—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        """
        try:
            # –≠—Ç–æ —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è
            title_elem = item.find('a', class_='doc-title')
            if not title_elem:
                return None

            title = title_elem.get_text(strip=True)
            url = urljoin(self.BASE_URL, title_elem.get('href', ''))

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –∏ –¥–∞—Ç—É
            doc_number = self._extract_doc_number(title)
            doc_date = self._extract_doc_date(item)

            # –°—Ç–∞—Ç—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞
            status_elem = item.find('span', class_='doc-status')
            status = status_elem.get_text(strip=True) if status_elem else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

            return {
                "title": title,
                "url": url,
                "number": doc_number,
                "date": doc_date,
                "status": status,
                "source": "adilet.zan.kz"
            }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ: {e}")
            return None

    def _extract_doc_number(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á—å –Ω–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–∏–ø–∞ "‚Ññ 123-VI" –∏–ª–∏ "N 123"
        patterns = [
            r'‚Ññ\s*(\d+(?:-[IVX]+)?)',
            r'N\s*(\d+(?:-[IVX]+)?)',
        ]

        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)

        return None

    def _extract_doc_date(self, item) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á—å –¥–∞—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            date_elem = item.find('span', class_='doc-date')
            if date_elem:
                return date_elem.get_text(strip=True)
        except Exception:
            pass

        return None

    def _extract_from_link(self, link) -> Optional[Dict[str, Any]]:
        """
        –ò–∑–≤–ª–µ—á—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –∏–∑ —Å—Å—ã–ª–∫–∏

        Args:
            link: BeautifulSoup —ç–ª–µ–º–µ–Ω—Ç —Å—Å—ã–ª–∫–∏

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        """
        try:
            href = link.get('href', '')
            if not href or not href.startswith('/rus/docs/'):
                return None

            url = urljoin(self.BASE_URL, href)
            title = link.get_text(strip=True)

            if not title or len(title) < 10:  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ - —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –Ω–µ —Ç–æ
                return None

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –∏–∑ title –∏–ª–∏ URL
            doc_number = self._extract_doc_number(title)
            if not doc_number:
                # –ü–æ–ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ URL
                url_match = re.search(r'/docs/([A-Z]\d+)', href)
                if url_match:
                    doc_number = url_match.group(1)

            # –ò—â–µ–º –¥–∞—Ç—É –≤ —Ç–µ–∫—Å—Ç–µ —Ä—è–¥–æ–º —Å —Å—Å—ã–ª–∫–æ–π
            parent = link.parent
            date_text = parent.get_text() if parent else title
            doc_date = self._extract_date_from_text(date_text)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é - –¥–µ–π—Å—Ç–≤—É–µ—Ç –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ –æ–±—Ä–∞—Ç–Ω–æ–µ)
            status = "–î–µ–π—Å—Ç–≤—É–µ—Ç"
            if parent and ("—É—Ç—Ä–∞—Ç–∏–ª" in parent.get_text().lower() or "–Ω–µ–¥–µ–π—Å—Ç–≤—É—é—â" in parent.get_text().lower()):
                status = "–£—Ç—Ä–∞—Ç–∏–ª —Å–∏–ª—É"

            return {
                "title": title,
                "url": url,
                "number": doc_number or "–ù–µ —É–∫–∞–∑–∞–Ω",
                "date": doc_date or "–ù–µ —É–∫–∞–∑–∞–Ω–∞",
                "status": status,
                "source": "adilet.zan.kz"
            }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑ —Å—Å—ã–ª–∫–∏: {e}")
            return None

    def _extract_date_from_text(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á—å –¥–∞—Ç—É –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ò—â–µ–º –¥–∞—Ç—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        date_patterns = [
            r'(\d{1,2}\.\d{1,2}\.\d{4})',  # –¥–¥.–º–º.–≥–≥–≥–≥
            r'(\d{1,2}\s+\w+\s+\d{4})',     # –¥–¥ –º–µ—Å—è—Ü –≥–≥–≥–≥
            r'–æ—Ç\s+(\d{1,2}\.\d{1,2}\.\d{4})',  # –æ—Ç –¥–¥.–º–º.–≥–≥–≥–≥
        ]

        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)

        return None

    def _get_demo_results(self, query: str) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å

        Returns:
            –î–µ–º–æ-—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        """
        logger.info("–í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")

        demo_results = [
            {
                "title": f"–ó–∞–∫–æ–Ω –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω - —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è '{query}'",
                "url": f"{self.BASE_URL}/rus/docs/example",
                "number": "123-VI",
                "date": "01.01.2024",
                "status": "–î–µ–π—Å—Ç–≤—É–µ—Ç",
                "source": "adilet.zan.kz (–¥–µ–º–æ)",
                "summary": "–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç. –í —Ä–µ–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å adilet.zan.kz"
            }
        ]

        return demo_results


class AdiletDocumentFetcher(BaseTool):
    """
    –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å adilet.zan.kz
    """

    name = "adilet_fetch_document"
    description = "–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –ù–ü–ê —Å adilet.zan.kz –ø–æ URL"
    parameters = {
        "type": "object",
        "properties": {
            "url": {
                "type": "string",
                "description": "URL –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ adilet.zan.kz"
            }
        },
        "required": ["url"]
    }

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        super().__init__()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def execute(self, url: str, **kwargs) -> Union[Dict[str, Any], ToolResult]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞

        Args:
            url: URL –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ adilet.zan.kz
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

        Returns:
            –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL —Å adilet.zan.kz
            if "adilet.zan.kz" not in url:
                return {
                    "status": "error",
                    "error": "URL –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å adilet.zan.kz"
                }

            logger.info(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {url}")

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
            # verify=False –¥–ª—è –æ–±—Ö–æ–¥–∞ –ø—Ä–æ–±–ª–µ–º —Å SSL —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–º adilet.zan.kz
            response = self.session.get(url, timeout=15, verify=False)
            response.raise_for_status()

            # –ü–∞—Ä—Å–∏–º –¥–æ–∫—É–º–µ–Ω—Ç
            document = self._parse_document(response.text, url)

            return {
                "status": "success",
                "url": url,
                "document": document,
                "source": "adilet.zan.kz"
            }

        except requests.RequestException as e:
            error_msg = f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {str(e)}"
            logger.error(error_msg)
            return {
                "status": "error",
                "error": error_msg
            }
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {str(e)}"
            logger.error(error_msg)
            return {
                "status": "error",
                "error": error_msg
            }

    def _parse_document(self, html: str, url: str) -> Dict[str, Any]:
        """
        –†–∞—Å–ø–∞—Ä—Å–∏—Ç—å HTML –¥–æ–∫—É–º–µ–Ω—Ç–∞

        Args:
            html: HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
            url: URL –¥–æ–∫—É–º–µ–Ω—Ç–∞

        Returns:
            –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        """
        try:
            soup = BeautifulSoup(html, 'html.parser')

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            title = self._extract_title(soup)
            doc_type = self._extract_doc_type(soup)
            number = self._extract_number(soup)
            date = self._extract_date(soup)
            status = self._extract_status(soup)
            text = self._extract_text(soup)

            return {
                "title": title,
                "type": doc_type,
                "number": number,
                "date": date,
                "status": status,
                "text": text,
                "url": url
            }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return {
                "title": "–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞",
                "text": "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                "error": str(e)
            }

    def _extract_title(self, soup) -> str:
        """–ò–∑–≤–ª–µ—á—å –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        title_elem = soup.find('h1') or soup.find('div', class_='doc-title')
        return title_elem.get_text(strip=True) if title_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

    def _extract_doc_type(self, soup) -> str:
        """–ò–∑–≤–ª–µ—á—å —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        # –õ–æ–≥–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        return "–ó–∞–∫–æ–Ω"  # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è

    def _extract_number(self, soup) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á—å –Ω–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        # –õ–æ–≥–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–º–µ—Ä–∞
        return None

    def _extract_date(self, soup) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á—å –¥–∞—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        # –õ–æ–≥–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞—Ç—ã
        return None

    def _extract_status(self, soup) -> str:
        """–ò–∑–≤–ª–µ—á—å —Å—Ç–∞—Ç—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        # –õ–æ–≥–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞
        return "–î–µ–π—Å—Ç–≤—É–µ—Ç"

    def _extract_text(self, soup) -> str:
        """–ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
        content = soup.find('div', class_='doc-content') or soup.find('div', class_='document-text')

        if content:
            # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö —Ç–µ–≥–æ–≤
            for script in content(['script', 'style']):
                script.decompose()

            return content.get_text(separator='\n', strip=True)

        return "–¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω"
